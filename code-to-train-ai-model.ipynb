{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":75676,"sourceType":"datasetVersion","datasetId":42780},{"sourceId":9261917,"sourceType":"datasetVersion","datasetId":5604285},{"sourceId":9262246,"sourceType":"datasetVersion","datasetId":5604455}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image\nimport os\nfrom pathlib import Path\n\n# Define the input and output directories\ninput_dir = Path('/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/images')\noutput_dir = Path('/kaggle/working/yy')\n\n# Create the output directory if it doesn't exist\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Loop through all files in the input directory\nfor img_path in input_dir.glob('*'):\n    if img_path.suffix.lower() in ['.png', '.jpeg', '.jfif']:\n        # Open the image\n        with Image.open(img_path) as img:\n            # Convert the image to RGB if it's not in that mode\n            img = img.convert('RGB')\n            \n            # Define the output path with .jpg extension\n            output_path = output_dir / (img_path.stem + '.jpg')\n            \n            # Save the image as .jpg\n            img.save(output_path, 'JPEG')\n\nprint(\"Conversion complete. All images have been saved as .jpg in the /kaggle/working/sp directory.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:03:03.322286Z","iopub.execute_input":"2024-08-28T08:03:03.322965Z","iopub.status.idle":"2024-08-28T08:05:53.646321Z","shell.execute_reply.started":"2024-08-28T08:03:03.322915Z","shell.execute_reply":"2024-08-28T08:05:53.645217Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Conversion complete. All images have been saved as .jpg in the /kaggle/working/sp directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\n\n# Define the paths\ninput_path = Path('/kaggle/working/yy')\noutput_path = Path('/kaggle/working/not_sp')\n\n# Create the output directory if it doesn't exist\noutput_path.mkdir(parents=True, exist_ok=True)\n\n# Get all image files from the input directory\nall_images = list(input_path.glob('**/*.jpg'))  # Use '**/*.jpg' to include subdirectories\n\n# Ensure that the number of images to be selected does not exceed the available images\nnum_images_to_select = min(1000, len(all_images))\n\n# Select random images\nselected_images = random.sample(all_images, num_images_to_select)\n\n# Copy the selected images to the output directory\nfor img_path in selected_images:\n    shutil.copy(img_path, output_path / img_path.name)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:05:53.648720Z","iopub.execute_input":"2024-08-28T08:05:53.649235Z","iopub.status.idle":"2024-08-28T08:05:53.998816Z","shell.execute_reply.started":"2024-08-28T08:05:53.649177Z","shell.execute_reply":"2024-08-28T08:05:53.997420Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\n\n# Define the paths\ninput_path = Path('/kaggle/input/natural-images/natural_images')\noutput_path = Path('/kaggle/working/not_sp')\n\n# Create the output directory if it doesn't exist\noutput_path.mkdir(parents=True, exist_ok=True)\n\n# Move 600 random images from the 'person' folder\nperson_folder = input_path / 'person'\nperson_images = list(person_folder.glob('*.jpg'))\nselected_person_images = random.sample(person_images, 100)\n\nfor img_path in selected_person_images:\n    shutil.copy(img_path, output_path / img_path.name)\n\n# Move 100 images from each of the other folders\nfor folder in input_path.iterdir():\n    if folder.is_dir() and folder.name != 'person':\n        images = list(folder.glob('*.jpg'))\n        selected_images = random.sample(images, 90)\n        \n        for img_path in selected_images:\n            shutil.copy(img_path, output_path / img_path.name)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:05:54.000595Z","iopub.execute_input":"2024-08-28T08:05:54.001027Z","iopub.status.idle":"2024-08-28T08:06:01.315181Z","shell.execute_reply.started":"2024-08-28T08:05:54.000984Z","shell.execute_reply":"2024-08-28T08:06:01.313618Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\nfrom pathlib import Path\n\n# Define the input and output directories\ninput_dir = Path('/kaggle/input/saimgs/sat')\noutput_dir = Path('/kaggle/working/sp')\n\n# Create the output directory if it doesn't exist\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Loop through all files in the input directory\nfor img_path in input_dir.glob('*'):\n    if img_path.suffix.lower() in ['.png', '.jpeg', '.jfif']:\n        # Open the image\n        with Image.open(img_path) as img:\n            # Convert the image to RGB if it's not in that mode\n            img = img.convert('RGB')\n            \n            # Define the output path with .jpg extension\n            output_path = output_dir / (img_path.stem + '.jpg')\n            \n            # Save the image as .jpg\n            img.save(output_path, 'JPEG')\n\nprint(\"Conversion complete. All images have been saved as .jpg in the /kaggle/working/sp directory.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:06:01.317458Z","iopub.execute_input":"2024-08-28T08:06:01.317841Z","iopub.status.idle":"2024-08-28T08:06:04.156570Z","shell.execute_reply.started":"2024-08-28T08:06:01.317799Z","shell.execute_reply":"2024-08-28T08:06:04.155427Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Conversion complete. All images have been saved as .jpg in the /kaggle/working/sp directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport cv2\nimport albumentations as A\n\n# Define augmentation pipeline with individual transformations\naugmentations = [\n    A.HorizontalFlip(p=1.0),  # Horizontal flip\n    A.VerticalFlip(p=1.0),  # Vertical flip\n    A.RandomRotate90(p=1.0),  # Random 90 degree rotations\n    A.Rotate(limit=30, p=1.0),  # Rotate within -30 to 30 degrees\n    A.Rotate(limit=60, p=1.0),  # Rotate within -60 to 60 degrees\n    A.Rotate(limit=(10, 170), p=1.0),  # Rotate within 10 to 170 degrees\n    A.Blur(blur_limit=3, p=1.0),  # Apply a slight blur\n    A.MedianBlur(blur_limit=3, p=1.0),  # Apply a median blur\n    A.CLAHE(clip_limit=4.0, p=1.0),  # Contrast Limited Adaptive Histogram Equalization\n    A.RandomBrightnessContrast(p=1.0),  # Random brightness and contrast adjustments\n    A.HueSaturationValue(p=1.0),  # Adjust hue, saturation, and value\n    A.RGBShift(p=1.0),  # Randomly shift RGB channels\n    A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), p=1.0),  # Random crop and resize\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=45, p=1.0),  # Shift, scale, and rotate\n    A.RandomGamma(p=1.0),  # Random gamma adjustments\n    A.GaussNoise(p=1.0),  # Add Gaussian noise\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=1.0),  # Random changes in brightness, contrast, saturation, and hue\n    A.Equalize(p=1.0),  # Equalize the image histogram\n    A.Solarize(threshold=128, p=1.0),  # Invert all pixel values above a threshold\n    A.ToSepia(p=1.0)  # Apply a sepia tone to the image\n]\n\n# Directories\ndirectories = ['/kaggle/working/not_sp', '/kaggle/working/sp']\n\nfor directory in directories:\n    image_files = list(Path(directory).glob('*.jpg'))\n    \n    for img_file in image_files:\n        image = cv2.imread(str(img_file))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        for i, aug in enumerate(augmentations):\n            # Apply the specific augmentation\n            augmented_image = aug(image=image)['image']\n            \n            # Save the augmented image with a new name\n            new_filename = os.path.join(directory, f'aug_{i}_{img_file.stem}.jpg')\n            augmented_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n            cv2.imwrite(new_filename, augmented_image)\n\nprint(\"Augmentation complete for both directories with multiple versions per image.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:06:15.330047Z","iopub.execute_input":"2024-08-28T08:06:15.330502Z","iopub.status.idle":"2024-08-28T08:18:37.871386Z","shell.execute_reply.started":"2024-08-28T08:06:15.330457Z","shell.execute_reply":"2024-08-28T08:18:37.870097Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Augmentation complete for both directories with multiple versions per image.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport cv2\n\n# Define the target size\ntarget_size = (128, 128)  # You can adjust this size as needed\n\n# Directories\ndirectories = ['/kaggle/working/not_sp', '/kaggle/working/sp']\n\nfor directory in directories:\n    image_files = list(Path(directory).glob('*.jpg'))\n    \n    for img_file in image_files:\n        image = cv2.imread(str(img_file))\n        \n        # Resize the image to the target size\n        resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n        \n        # Save the resized image, replacing the original\n        cv2.imwrite(str(img_file), resized_image)\n\nprint(\"All images have been resized to the target size in both directories.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:18:37.873732Z","iopub.execute_input":"2024-08-28T08:18:37.874308Z","iopub.status.idle":"2024-08-28T08:27:21.383613Z","shell.execute_reply.started":"2024-08-28T08:18:37.874261Z","shell.execute_reply":"2024-08-28T08:27:21.382546Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"All images have been resized to the target size in both directories.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Define the directories\nsp_dir = Path('/kaggle/working/sp')\nnot_sp_dir = Path('/kaggle/working/not_sp')\n\n# Collect all image paths and assign labels\ndata = []\n\n# Label all images in sp folder as 1\nfor img_path in sp_dir.glob('*.jpg'):\n    data.append({'image_path': str(img_path), 'label': 1})\n\n# Label all images in not_sp folder as 0\nfor img_path in not_sp_dir.glob('*.jpg'):\n    data.append({'image_path': str(img_path), 'label': 0})\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Print the number of images with each label\nprint(df['label'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:21.385294Z","iopub.execute_input":"2024-08-28T08:27:21.385607Z","iopub.status.idle":"2024-08-28T08:27:22.613894Z","shell.execute_reply.started":"2024-08-28T08:27:21.385571Z","shell.execute_reply":"2024-08-28T08:27:22.612830Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"label\n0    36330\n1    13188\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform train/validation split\ntrain_df, valid_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n\n# Print the sizes of the train and validation sets\nprint(f\"Training set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(valid_df)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:22.616604Z","iopub.execute_input":"2024-08-28T08:27:22.617068Z","iopub.status.idle":"2024-08-28T08:27:22.656800Z","shell.execute_reply.started":"2024-08-28T08:27:22.617033Z","shell.execute_reply":"2024-08-28T08:27:22.655749Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training set size: 39614\nValidation set size: 9904\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save train and validation data to CSV files\ntrain_df.to_csv('/kaggle/working/train_data.csv', index=False)\nvalid_df.to_csv('/kaggle/working/valid_data.csv', index=False)\n\nprint(\"Train and validation data saved to CSV files.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:22.658992Z","iopub.execute_input":"2024-08-28T08:27:22.659868Z","iopub.status.idle":"2024-08-28T08:27:22.836282Z","shell.execute_reply.started":"2024-08-28T08:27:22.659803Z","shell.execute_reply":"2024-08-28T08:27:22.835199Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Train and validation data saved to CSV files.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:22.838061Z","iopub.execute_input":"2024-08-28T08:27:22.838565Z","iopub.status.idle":"2024-08-28T08:27:35.522749Z","shell.execute_reply.started":"2024-08-28T08:27:22.838500Z","shell.execute_reply":"2024-08-28T08:27:35.521901Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Assuming you have train_df and valid_df from the previous steps\ntrain_df = pd.read_csv('/kaggle/working/train_data.csv')\nvalid_df = pd.read_csv('/kaggle/working/valid_data.csv')\n# Convert the labels to strings in both train and validation DataFrames\ntrain_df['label'] = train_df['label'].astype(str)\nvalid_df['label'] = valid_df['label'].astype(str)\n\n# Define ImageDataGenerator for data augmentation and preprocessing\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,  # Rescale pixel values to [0, 1]\n\n)\n\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\n# Create generators\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    x_col='image_path',\n    y_col='label',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary'\n)\n\nvalid_generator = valid_datagen.flow_from_dataframe(\n    valid_df,\n    x_col='image_path',\n    y_col='label',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:35.524216Z","iopub.execute_input":"2024-08-28T08:27:35.524994Z","iopub.status.idle":"2024-08-28T08:27:36.219943Z","shell.execute_reply.started":"2024-08-28T08:27:35.524946Z","shell.execute_reply":"2024-08-28T08:27:36.218886Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Found 39614 validated image filenames belonging to 2 classes.\nFound 9904 validated image filenames belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the base model with pre-trained weights\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n\n# Alternatively, use ResNet50\n# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n\n# Freeze the base model initially\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nmodel = models.Sequential([\n    base_model,\n    layers.GlobalAveragePooling2D(),  # Pool the feature maps\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),  # Add dropout for regularization\n    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n])\n\n# Compile the model with the base model frozen\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Initial model summary with the base model frozen\nprint(\"Model summary with base model frozen:\")\nmodel.summary()\n\n# Unfreeze some layers of the base model for fine-tuning\n# For example, unfreeze the last 20 layers\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\n# Re-compile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Model summary after unfreezing some layers\nprint(\"Model summary after unfreezing the last 20 layers:\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:27:36.221173Z","iopub.execute_input":"2024-08-28T08:27:36.221531Z","iopub.status.idle":"2024-08-28T08:27:38.704397Z","shell.execute_reply.started":"2024-08-28T08:27:36.221497Z","shell.execute_reply":"2024-08-28T08:27:38.703370Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nModel summary with base model frozen:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ ?                      │     \u001b[38;5;34m4,049,571\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model summary after unfreezing the last 20 layers:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ ?                      │     \u001b[38;5;34m4,049,571\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,350,960\u001b[0m (5.15 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,350,960</span> (5.15 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,698,611\u001b[0m (10.29 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,698,611</span> (10.29 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Train the model with the frozen base model\nhistory = model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=5  # Adjust the number of epochs as needed\n)\n\n# Unfreeze some layers of the base model for fine-tuning\nbase_model.trainable = True\n\n# Re-compile the model with a lower learning rate for fine-tuning\nmodel.compile(optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Continue training with fine-tuning\nhistory_fine = model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=5  # Continue for additional epochs\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T08:31:39.947562Z","iopub.execute_input":"2024-08-28T08:31:39.948377Z","iopub.status.idle":"2024-08-28T08:44:27.562346Z","shell.execute_reply.started":"2024-08-28T08:31:39.948334Z","shell.execute_reply":"2024-08-28T08:44:27.561414Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 41ms/step - accuracy: 0.7332 - loss: 0.5862 - val_accuracy: 0.7336 - val_loss: 0.5798\nEpoch 2/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.7339 - loss: 0.5842 - val_accuracy: 0.7336 - val_loss: 0.5792\nEpoch 3/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 34ms/step - accuracy: 0.7347 - loss: 0.5821 - val_accuracy: 0.7336 - val_loss: 0.5796\nEpoch 4/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.7354 - loss: 0.5807 - val_accuracy: 0.7336 - val_loss: 0.5789\nEpoch 5/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.7334 - loss: 0.5823 - val_accuracy: 0.7336 - val_loss: 0.5794\nEpoch 1/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 111ms/step - accuracy: 0.7966 - loss: 0.4498 - val_accuracy: 0.9764 - val_loss: 0.1025\nEpoch 2/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9663 - loss: 0.1041 - val_accuracy: 0.9910 - val_loss: 0.0380\nEpoch 3/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9815 - loss: 0.0533 - val_accuracy: 0.9940 - val_loss: 0.0215\nEpoch 4/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9893 - loss: 0.0341 - val_accuracy: 0.9952 - val_loss: 0.0145\nEpoch 5/5\n\u001b[1m1238/1238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9928 - loss: 0.0224 - val_accuracy: 0.9959 - val_loss: 0.0116\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the entire model to a file\nmodel.save('/kaggle/working/my_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T01:13:22.010693Z","iopub.status.idle":"2024-08-28T01:13:22.011080Z","shell.execute_reply.started":"2024-08-28T01:13:22.010896Z","shell.execute_reply":"2024-08-28T01:13:22.010916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}